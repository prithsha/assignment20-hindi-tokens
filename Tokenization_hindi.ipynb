{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "# Important reference : https://github.com/karpathy/minbpe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Content of c:\\Users\\1430647\\My\\repo\\github\\assignment20-hindi-tokens\\hindi_input.txt:\n",
      "भारत के संविधान का स्वरूप गणतंत्रीय तथा ढांचा संघीय है और उसमें संसदीय प्रणाली के प्रमुख तत्व विद्यमान हैं । इसमें संघ के लिये एक संसद का प्रावधान है जिसमें राष्ट्रपति और दो सदन अर्थात् राज्य सभा ( काउंसिल ऑफ स्टेट्स ) और लोक सभा ( हाउस ऑफ दी पीपल ) सम्मिलित हैं ; इसमें संघ की कार्यपालिका का भी प्रावधान है जो संसद के दोनों सदनों के सदस्यों में से सदस्य लेकर बनती है और वह सामूहिक रूप से लोक सभा के प्रति उत्तरदायी होती है , इस प्रकार संघ की कार्यपालिका और संसद के बीच घनिष्ठ संबंध सुनिश्चित हो जाता है ;\n",
      "इसमें यह भी प्रावधान है कि एक राज्याध्यक्ष होगा जिसे भारत का राष्ट्रपति कहा जाएगा और वह केन्द्रीय मंत्रिपरिषद की सहायता तथा सलाह से काम करेगा ; भारतीय संविधान में अनेक राज्यों का प्रावधान है जिनकी कार्यपालिकाओं और राज्य विधानमंडलों के बारे में वैसे ही मूल उपबन्ध हैं जैसे कि संघ के बारे में हैं ; संविधान में विधि सम्मत शासन की व्यवस्था है तथा इसमें एक स्वतंत्र न्यायपालिका की और एक स्थायी सिविल सेवा की व्यवस्था है । भारत की संसद प्रभुसत्ता सम्पन्न निकाय नहीं है , यह एक लिखित संविधान की सीमाओं के अन्तर्गत कार्य करती है ।\n",
      "इसके विधायी प्राधिकार पर दो प्रकार की सीमाएं हैं , एक तो यह कि संघ और राज्यों के बीच शक्तियों का बंटवारा किया गया है और दूसरी यह कि संविधान में न्याय्य मौलिक अधिकारों का समावेश है तथा न्यायिक पुनरीक्षण का प्रावधान है जिसका अर्थ यह है कि संसद द्वारा पारित सभी विधियां अनिवार्यतः संविधान के उपबन्धों के अनुसार होनी चाहिए और उनकी संवैधानिकता की जांच एक स्वतंत्र न्यायपालिका द्वारा की जा सकती है । इन सब उपबंधों से संसद के प्राधिकार तथा अधिकार क्षेत्र के स्वरूप तथा विस्तार का पता चलता है ।\n",
      "• भारत की सबसे बड़ी कानून बनाने वाली सभा संसद है, जिसमें राष्ट्रपति, लोकसभा और राज्य सभा सम्मिलित हैं।\n",
      "• एडविन लुटियंस और हर्बर्ट बेकर नामक वास्तुकारों ने संसद भवन की रूपरेखा तैयार की थी। संसद भवन के निर्माण हेतु शिलान्यास 12 फरवरी, 1921 को किया गया था। इसका निर्माण कार्य छह वर्ष बाद 18 जनवरी 1927 को पूरा हुआ था।\n",
      "• संसद भवन का उद्घाटन तत्कालीन भारतीय वायसराय लॉर्ड इरविन ने किया था। संसद परिसर का क्षेत्रफल लगभग 6 एकड़ है। संसद में प्रवेश के लिए 12 दरवाजे हैं। केन्द्रीय कक्ष संसद \n",
      "...\n"
     ]
    }
   ],
   "source": [
    "file_path = os.path.join(os.getcwd(), \"hindi_input.txt\")\n",
    "\n",
    "def get_file_input():\n",
    "\n",
    "    file_content = None\n",
    "    try:\n",
    "        with open(file_path, \"r\") as file:\n",
    "            file_content = file.read()            \n",
    "    except FileNotFoundError:\n",
    "        print(f\"File {file_path} not found.\")\n",
    "    return file_content\n",
    "\n",
    "file_content = get_file_input()\n",
    "print(f\"Content of {file_path}:\\n{file_content[:2000]}\\n...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text length: 260799, Total token length: 661827, First 20 tokens: [224, 164, 173, 224, 164, 190, 224, 164, 176, 224, 164, 164, 32, 224, 164, 149, 224, 165, 135, 32]...\n"
     ]
    }
   ],
   "source": [
    "tokens = file_content.encode(\"utf-8\")\n",
    "tokens = list(map(int, tokens))\n",
    "print(f\"Text length: {len(file_content)}, Total token length: {len(tokens)}, First 20 tokens: {tokens[:20]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_line = \"भारत का राष्ट्र गान । राष्ट्र गान के बारे में समय समय पर अनुदेश जारी किए गए हैं, इनमें वे अवसर जिन पर इसे बजाया गाया। राज्यसभा के कुल सदस्य 250 होते हैं। इन 250 सदस्यों में से 12 सदस्य राष्ट्रपति द्वारा मनोनीत किए जाते हैं। 1 फरवरी, 2021 को, वित्त मंत्री कार्यकाल : (नियुक्ति की तारीख) 13-05-2016 से (निवृत्ति की तारीख) 10-11-2024 @| लेग बिफोर विकेट (Leg before wicket)(एल बी डब्ल्यू); यह जटिल है लेकिन इसका मूल अर्थ यह होता है कि यदि गेंद ने पहले बल्लेबाज की टांग को न छुआ होता तो वो आउट हो जाता[13] (एक दिवसीय मैच में सामान्यत: ५० ओवर और ट्वेंटी 20 में सामान्यत:२० ओवर| पुराने विचारों के हिन्दू ‘रामायण’ को “त्रेता” युग की तथा महाभारत को “द्वापर” युग की कृति मानते हैं. https://srweb.in/indian-history/ancient-india/)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['भारत', ' का', ' राष्ट्र', ' गान', ' राष्ट्र', ' गान', ' के', ' बारे', ' में', ' समय', ' समय', ' पर', ' अनुदेश', ' जारी', ' किए', ' गए', ' इनमें', ' वे', ' अवसर', ' जिन', ' पर', ' इसे', ' बजाया', ' राज्यसभा', ' के', ' कुल', ' सदस्य', '250', ' होते', ' इन', '250', ' सदस्यों', ' में', ' से', '12', ' सदस्य', ' राष्ट्रपति', ' द्वारा', ' मनोनीत', ' किए', ' जाते', '1', '2021', ' वित्त', ' मंत्री', ' कार्यकाल', ' की', '13', '05', '2016', ' से', ' की', '10', '11', '2024', ' लेग', ' बिफोर', ' विकेट', ' बी', ' यह', ' जटिल', ' है', ' लेकिन', ' इसका', ' मूल', ' अर्थ', ' यह', ' होता', ' है', ' कि', ' यदि', ' गेंद', ' ने', ' पहले', ' बल्लेबाज', ' की', ' टांग', ' को', ' न', ' छुआ', ' होता', ' तो', ' वो', ' आउट', ' हो', '13', ' दिवसीय', ' मैच', ' में', ' ५०', ' ओवर', ' और', ' ट्वेंटी', '20', ' में', '२०', ' पुराने', ' विचारों', ' के', ' हिन्दू', ' को', ' युग', ' की', ' तथा', ' महाभारत', ' को', ' युग', ' की', ' कृति', ' मानते']\n",
      "['भारत', ' का', ' राष्ट्र', ' गान', ' राष्ट्र', ' गान', ' के', ' बारे', ' में', ' समय', ' समय', ' पर', ' अनुदेश', ' जारी', ' किए', ' गए', 'हैं,', ' इनमें', ' वे', ' अवसर', ' जिन', ' पर', ' इसे', ' बजाया', ' राज्यसभा', ' के', ' कुल', ' सदस्य', '250', ' होते', ' इन', '250', ' सदस्यों', ' में', ' से', '12', ' सदस्य', ' राष्ट्रपति', ' द्वारा', ' मनोनीत', ' किए', ' जाते', '1', 'फरवरी,', '2021', 'को,', ' वित्त', ' मंत्री', ' कार्यकाल', ' की', '13', '05', '2016', ' से', ' की', '10', '11', '2024', ' लेग', ' बिफोर', ' विकेट', ' बी', ' यह', ' जटिल', ' है', ' लेकिन', ' इसका', ' मूल', ' अर्थ', ' यह', ' होता', ' है', ' कि', ' यदि', ' गेंद', ' ने', ' पहले', ' बल्लेबाज', ' की', ' टांग', ' को', ' न', ' छुआ', ' होता', ' तो', ' वो', ' आउट', ' हो', '13', ' दिवसीय', ' मैच', ' में', ' ५०', ' ओवर', ' और', ' ट्वेंटी', '20', ' में', '२०', ' पुराने', ' विचारों', ' के', ' हिन्दू', ' को', ' युग', ' की', ' तथा', ' महाभारत', ' को', ' युग', ' की', ' कृति', ' मानते']\n",
      "['भारत', ' का', ' राष्ट्र', ' गान', ' ', ' राष्ट्र', ' गान', ' के', ' बारे', ' में', ' समय', ' समय', ' पर', ' अनुदेश', ' जारी', ' किए', ' गए', ' ', 'हैं', ' इनमें', ' वे', ' अवसर', ' जिन', ' पर', ' इसे', ' बजाया', ' ', 'गाया', ' राज्यसभा', ' के', ' कुल', ' सदस्य', ' ', '250', ' होते', ' ', 'हैं', ' इन', ' ', '250', ' सदस्यों', ' में', ' से', ' ', '12', ' सदस्य', ' राष्ट्रपति', ' द्वारा', ' मनोनीत', ' किए', ' जाते', ' ', 'हैं', ' ', '1', ' ', 'फरवरी', ' ', '2021', ' ', 'को', ' वित्त', ' मंत्री', ' कार्यकाल', ' ', ' ', 'नियुक्ति', ' की', ' ', 'तारीख', ' ', '13', '05', '2016', ' से', ' ', 'निवृत्ति', ' की', ' ', 'तारीख', ' ', '10', '11', '2024', ' ', ' लेग', ' बिफोर', ' विकेट', ' ', ' ', 'before', ' ', 'wicket', 'एल', ' बी', ' ', 'डब्ल्यू', ' यह', ' जटिल', ' है', ' लेकिन', ' इसका', ' मूल', ' अर्थ', ' यह', ' होता', ' है', ' कि', ' यदि', ' गेंद', ' ने', ' पहले', ' बल्लेबाज', ' की', ' टांग', ' को', ' न', ' छुआ', ' होता', ' तो', ' वो', ' आउट', ' हो', ' ', 'जाता', '13', ' ', 'एक', ' दिवसीय', ' मैच', ' में', ' ', 'सामान्यत', ' ५०', ' ओवर', ' और', ' ट्वेंटी', ' ', '20', ' में', ' ', 'सामान्यत', '२०', ' ', 'ओवर', ' पुराने', ' विचारों', ' के', ' हिन्दू', ' ', 'रामायण', ' को', ' ', 'त्रेता', ' युग', ' की', ' तथा', ' महाभारत', ' को', ' ', 'द्वापर', ' युग', ' की', ' कृति', ' मानते', ' ', 'हैं', ' ', 'https']\n",
      "['भारत', ' का', ' राष्ट्र', ' गान', ' ', '।', ' राष्ट्र', ' गान', ' के', ' बारे', ' में', ' समय', ' समय', ' पर', ' अनुदेश', ' जारी', ' किए', ' गए', ' ', 'हैं', ',', ' इनमें', ' वे', ' अवसर', ' जिन', ' पर', ' इसे', ' बजाया', ' ', 'गाया', '।', ' राज्यसभा', ' के', ' कुल', ' सदस्य', ' ', '250', ' होते', ' ', 'हैं', '।', ' इन', ' ', '250', ' सदस्यों', ' में', ' से', ' ', '12', ' सदस्य', ' राष्ट्रपति', ' द्वारा', ' मनोनीत', ' किए', ' जाते', ' ', 'हैं', '।', ' ', '1', ' ', 'फरवरी', ',', ' ', '2021', ' ', 'को', ',', ' वित्त', ' मंत्री', ' कार्यकाल', ' ', ':', ' ', '(', 'नियुक्ति', ' की', ' ', 'तारीख', ')', ' ', '13', '-', '05', '-', '2016', ' से', ' ', '(', 'निवृत्ति', ' की', ' ', 'तारीख', ')', ' ', '10', '-', '11', '-', '2024', ' ', '@|', ' लेग', ' बिफोर', ' विकेट', ' ', '(', ' ', ' ', ')(', 'एल', ' बी', ' ', 'डब्ल्यू', ');', ' यह', ' जटिल', ' है', ' लेकिन', ' इसका', ' मूल', ' अर्थ', ' यह', ' होता', ' है', ' कि', ' यदि', ' गेंद', ' ने', ' पहले', ' बल्लेबाज', ' की', ' टांग', ' को', ' न', ' छुआ', ' होता', ' तो', ' वो', ' आउट', ' हो', ' ', 'जाता', '[', '13', ']', ' ', '(', 'एक', ' दिवसीय', ' मैच', ' में', ' ', 'सामान्यत', ':', ' ५०', ' ओवर', ' और', ' ट्वेंटी', ' ', '20', ' में', ' ', 'सामान्यत', ':', '२०', ' ', 'ओवर', '|', ' पुराने', ' विचारों', ' के', ' हिन्दू', ' ', '‘', 'रामायण', '’', ' को', ' ', '“', 'त्रेता', '”', ' युग', ' की', ' तथा', ' महाभारत', ' को', ' ', '“', 'द्वापर', '”', ' युग', ' की', ' कृति', ' मानते', ' ', 'हैं', '.', ' ', '://', '.', '/', '-', '/', '-', '/)']\n",
      "['भारत', ' का', ' राष्ट्र', ' गान', ' ', '।', ' राष्ट्र', ' गान', ' के', ' बारे', ' में', ' समय', ' समय', ' पर', ' अनुदेश', ' जारी', ' किए', ' गए', ' ', 'हैं', ',', ' इनमें', ' वे', ' अवसर', ' जिन', ' पर', ' इसे', ' बजाया', ' ', 'गाया', '।', ' राज्यसभा', ' के', ' कुल', ' सदस्य', ' ', '250', ' होते', ' ', 'हैं', '।', ' इन', ' ', '250', ' सदस्यों', ' में', ' से', ' ', '12', ' सदस्य', ' राष्ट्रपति', ' द्वारा', ' मनोनीत', ' किए', ' जाते', ' ', 'हैं', '।', ' ', '1', ' ', 'फरवरी', ',', ' ', '2021', ' ', 'को', ',', ' वित्त', ' मंत्री', ' कार्यकाल', ' ', ':', ' ', '(', 'नियुक्ति', ' की', ' ', 'तारीख', ')', ' ', '13', '-', '05', '-', '2016', ' से', ' ', '(', 'निवृत्ति', ' की', ' ', 'तारीख', ')', ' ', '10', '-', '11', '-', '2024', ' ', '@|', ' लेग', ' बिफोर', ' विकेट', ' ', '(', ' ', 'before', ' ', 'wicket', ')(', 'एल', ' बी', ' ', 'डब्ल्यू', ');', ' यह', ' जटिल', ' है', ' लेकिन', ' इसका', ' मूल', ' अर्थ', ' यह', ' होता', ' है', ' कि', ' यदि', ' गेंद', ' ने', ' पहले', ' बल्लेबाज', ' की', ' टांग', ' को', ' न', ' छुआ', ' होता', ' तो', ' वो', ' आउट', ' हो', ' ', 'जाता', '[', '13', ']', ' ', '(', 'एक', ' दिवसीय', ' मैच', ' में', ' ', 'सामान्यत', ':', ' ५०', ' ओवर', ' और', ' ट्वेंटी', ' ', '20', ' में', ' ', 'सामान्यत', ':', '२०', ' ', 'ओवर', '|', ' पुराने', ' विचारों', ' के', ' हिन्दू', ' ', '‘', 'रामायण', '’', ' को', ' ', '“', 'त्रेता', '”', ' युग', ' की', ' तथा', ' महाभारत', ' को', ' ', '“', 'द्वापर', '”', ' युग', ' की', ' कृति', ' मानते', ' ', 'हैं', '.', ' ', 'https', '://', '.', '/', '-', '/', '-', '/)']\n"
     ]
    }
   ],
   "source": [
    "## Evaluating different Regex\n",
    "import regex as re\n",
    "# SPLIT_PATTERN = r\"\"\"(?:\\s+|^)\\p{Deva}+(?=\\s|$)\"\"\"\n",
    "SPLIT_PATTERN_1 = r\"\"\"(?:\\s+|^)\\p{Deva}+(?=\\s|$)|\\p{N}+\"\"\"\n",
    "SPLIT_PATTERN_2 = r\"\"\"(?:\\s+|^)\\p{Deva}+(?=\\s|$)|\\p{N}+|\\p{Deva}+,\"\"\"\n",
    "SPLIT_PATTERN_3 = r\"\"\"(?:\\s+|^)\\p{Deva}+(?=\\s|$)|\\p{N}+|\\p{Deva}+|(?<=\\s)\\p{L}+|\\s+(?!\\S)|\\s+\"\"\"\n",
    "SPLIT_PATTERN_4 = r\"\"\"(?:\\s+|^)\\p{Deva}+(?=\\s|$)|\\p{N}+|\\p{Deva}+|[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\"\n",
    "SPLIT_PATTERN_5 = r\"\"\"(?:\\s+|^)\\p{Deva}+(?=\\s|$)|\\p{N}+|\\p{Deva}+|(?<=\\s)\\p{L}+|[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\"\n",
    "compiled_pattern = re.compile(SPLIT_PATTERN_1)\n",
    "compiled_pattern_2 = re.compile(SPLIT_PATTERN_2)\n",
    "compiled_pattern_3 = re.compile(SPLIT_PATTERN_3)\n",
    "compiled_pattern_4 = re.compile(SPLIT_PATTERN_4)\n",
    "compiled_pattern_5 = re.compile(SPLIT_PATTERN_5)\n",
    "print(re.findall(compiled_pattern, single_line))\n",
    "print(re.findall(compiled_pattern_2, single_line))\n",
    "print(re.findall(compiled_pattern_3, single_line))\n",
    "print(re.findall(compiled_pattern_4, single_line))\n",
    "print(re.findall(compiled_pattern_5, single_line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# a few helper functions useful for both BasicTokenizer and RegexTokenizer\n",
    "\n",
    "def get_stats(ids, counts=None):\n",
    "    \"\"\"\n",
    "    Given a list of integers, return a dictionary of counts of consecutive pairs\n",
    "    Example: [1, 2, 3, 1, 2] -> {(1, 2): 2, (2, 3): 1, (3, 1): 1}\n",
    "    Optionally allows to update an existing dictionary of counts\n",
    "    \"\"\"\n",
    "    counts = {} if counts is None else counts\n",
    "    for pair in zip(ids, ids[1:]): # iterate consecutive elements\n",
    "        counts[pair] = counts.get(pair, 0) + 1\n",
    "    return counts\n",
    "\n",
    "\n",
    "def merge(ids, pair, idx):\n",
    "    \"\"\"\n",
    "    In the list of integers (ids), replace all consecutive occurrences\n",
    "    of pair with the new integer token idx\n",
    "    Example: ids=[1, 2, 3, 1, 2], pair=(1, 2), idx=4 -> [4, 3, 4]\n",
    "    \"\"\"\n",
    "    newids = []\n",
    "    i = 0\n",
    "    while i < len(ids):\n",
    "        # if not at the very last position AND the pair matches, replace it\n",
    "        if ids[i] == pair[0] and i < len(ids) - 1 and ids[i+1] == pair[1]:\n",
    "            newids.append(idx)\n",
    "            i += 2\n",
    "        else:\n",
    "            newids.append(ids[i])\n",
    "            i += 1\n",
    "    return newids\n",
    "\n",
    "# first two helper functions...\n",
    "def replace_control_characters(s: str) -> str:\n",
    "    # we don't want to print control characters\n",
    "    # which distort the output (e.g. \\n or much worse)\n",
    "    # https://stackoverflow.com/questions/4324790/removing-control-characters-from-a-string-in-python/19016117#19016117\n",
    "    # http://www.unicode.org/reports/tr44/#GC_Values_Table\n",
    "    chars = []\n",
    "    for ch in s:\n",
    "        if unicodedata.category(ch)[0] != \"C\":\n",
    "            chars.append(ch) # this character is ok\n",
    "        else:\n",
    "            chars.append(f\"\\\\u{ord(ch):04x}\") # escape\n",
    "    return \"\".join(chars)\n",
    "\n",
    "def render_token(t: bytes) -> str:\n",
    "    # pretty print a token, escaping control characters\n",
    "    s = t.decode('utf-8', errors='replace')\n",
    "    s = replace_control_characters(s)\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenizer:\n",
    "    \"\"\"Base class for Tokenizers\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        # default: vocab size of 256 (all bytes), no merges, no patterns\n",
    "        self.merges = {} # (int, int) -> int\n",
    "        self.pattern = \"\" # str\n",
    "        self.special_tokens = {} # str -> int, e.g. {'<|endoftext|>': 100257}\n",
    "        self.vocab = self._build_vocab() # int -> bytes\n",
    "\n",
    "    def train(self, text, vocab_size, verbose=False):\n",
    "        # Tokenizer can train a vocabulary of size vocab_size from text\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def encode(self, text):\n",
    "        # Tokenizer can encode a string into a list of integers\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def decode(self, ids):\n",
    "        # Tokenizer can decode a list of integers into a string\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def _build_vocab(self):\n",
    "        # vocab is simply and deterministically derived from merges\n",
    "        vocab = {idx: bytes([idx]) for idx in range(256)}\n",
    "        for (p0, p1), idx in self.merges.items():\n",
    "            vocab[idx] = vocab[p0] + vocab[p1]\n",
    "        for special, idx in self.special_tokens.items():\n",
    "            vocab[idx] = special.encode(\"utf-8\")\n",
    "        return vocab\n",
    "\n",
    "    def save(self, file_prefix):\n",
    "        \"\"\"\n",
    "        Saves two files: file_prefix.vocab and file_prefix.model\n",
    "        This is inspired (but not equivalent to!) sentencepiece's model saving:\n",
    "        - model file is the critical one, intended for load()\n",
    "        - vocab file is just a pretty printed version for human inspection only\n",
    "        \"\"\"\n",
    "        # write the model: to be used in load() later\n",
    "        model_file = file_prefix + \".model\"\n",
    "        with open(model_file, 'w') as f:\n",
    "            # write the version, pattern and merges, that's all that's needed\n",
    "            f.write(\"minbpe v1\\n\")\n",
    "            f.write(f\"{self.pattern}\\n\")\n",
    "            # write the special tokens, first the number of them, then each one\n",
    "            f.write(f\"{len(self.special_tokens)}\\n\")\n",
    "            for special, idx in self.special_tokens.items():\n",
    "                f.write(f\"{special} {idx}\\n\")\n",
    "            # the merges dict\n",
    "            for idx1, idx2 in self.merges:\n",
    "                f.write(f\"{idx1} {idx2}\\n\")\n",
    "        # write the vocab: for the human to look at\n",
    "        vocab_file = file_prefix + \".vocab\"\n",
    "        inverted_merges = {idx: pair for pair, idx in self.merges.items()}\n",
    "        with open(vocab_file, \"w\", encoding=\"utf-8\") as f:\n",
    "            for idx, token in self.vocab.items():\n",
    "                # note: many tokens may be partial utf-8 sequences\n",
    "                # and cannot be decoded into valid strings. Here we're using\n",
    "                # errors='replace' to replace them with the replacement char �.\n",
    "                # this also means that we couldn't possibly use .vocab in load()\n",
    "                # because decoding in this way is a lossy operation!\n",
    "                s = render_token(token)\n",
    "                # find the children of this token, if any\n",
    "                if idx in inverted_merges:\n",
    "                    # if this token has children, render it nicely as a merge\n",
    "                    idx0, idx1 = inverted_merges[idx]\n",
    "                    s0 = render_token(self.vocab[idx0])\n",
    "                    s1 = render_token(self.vocab[idx1])\n",
    "                    f.write(f\"[{s0}][{s1}] -> [{s}] {idx}\\n\")\n",
    "                else:\n",
    "                    # otherwise this is leaf token, just print it\n",
    "                    # (this should just be the first 256 tokens, the bytes)\n",
    "                    f.write(f\"[{s}] {idx}\\n\")\n",
    "\n",
    "    def load(self, model_file):\n",
    "        \"\"\"Inverse of save() but only for the model file\"\"\"\n",
    "        assert model_file.endswith(\".model\")\n",
    "        # read the model file\n",
    "        merges = {}\n",
    "        special_tokens = {}\n",
    "        idx = 256\n",
    "        with open(model_file, 'r', encoding=\"utf-8\") as f:\n",
    "            # read the version\n",
    "            version = f.readline().strip()\n",
    "            assert version == \"minbpe v1\"\n",
    "            # read the pattern\n",
    "            self.pattern = f.readline().strip()\n",
    "            # read the special tokens\n",
    "            num_special = int(f.readline().strip())\n",
    "            for _ in range(num_special):\n",
    "                special, special_idx = f.readline().strip().split()\n",
    "                special_tokens[special] = int(special_idx)\n",
    "            # read the merges\n",
    "            for line in f:\n",
    "                idx1, idx2 = map(int, line.split())\n",
    "                merges[(idx1, idx2)] = idx\n",
    "                idx += 1\n",
    "        self.merges = merges\n",
    "        self.special_tokens = special_tokens\n",
    "        self.vocab = self._build_vocab()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import regex as re\n",
    "\n",
    "# the main GPT text split patterns, see\n",
    "# https://github.com/openai/tiktoken/blob/main/tiktoken_ext/openai_public.py\n",
    "GPT2_SPLIT_PATTERN = r\"\"\"'(?:[sdmt]|ll|ve|re)| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\"\n",
    "GPT4_SPLIT_PATTERN = r\"\"\"'(?i:[sdmt]|ll|ve|re)|[^\\r\\n\\p{L}\\p{N}]?+\\p{L}+|\\p{N}{1,3}| ?[^\\s\\p{L}\\p{N}]++[\\r\\n]*|\\s*[\\r\\n]|\\s+(?!\\S)|\\s+\"\"\"\n",
    "\n",
    "class RegexTokenizer(Tokenizer):\n",
    "\n",
    "    def __init__(self, pattern=None):\n",
    "        \"\"\"\n",
    "        - pattern: optional string to override the default (GPT-4 split pattern)\n",
    "        - special_tokens: str -> int dictionary of special tokens\n",
    "          example: {'<|endoftext|>': 100257}\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.pattern = GPT4_SPLIT_PATTERN if pattern is None else pattern\n",
    "        self.compiled_pattern = re.compile(self.pattern)\n",
    "        self.special_tokens = {}\n",
    "        self.inverse_special_tokens = {}\n",
    "        self.train_ids_count = 0\n",
    "\n",
    "    def train(self, text, vocab_size, verbose=False):\n",
    "        assert vocab_size >= 256\n",
    "        num_merges = vocab_size - 256\n",
    "\n",
    "        # split the text up into text chunks\n",
    "        text_chunks = re.findall(self.compiled_pattern, text)\n",
    "\n",
    "        # input text preprocessing\n",
    "        ids = [list(ch.encode(\"utf-8\")) for ch in text_chunks]\n",
    "\n",
    "        # iteratively merge the most common pairs to create new tokens\n",
    "        merges = {} # (int, int) -> int\n",
    "        vocab = {idx: bytes([idx]) for idx in range(256)} # idx -> bytes\n",
    "        for i in range(num_merges):\n",
    "            # count the number of times every consecutive pair appears\n",
    "            stats = {}\n",
    "            for chunk_ids in ids:\n",
    "                # passing in stats will update it in place, adding up counts\n",
    "                get_stats(chunk_ids, stats)\n",
    "            # find the pair with the highest count\n",
    "            pair = max(stats, key=stats.get)\n",
    "            # mint a new token: assign it the next available id\n",
    "            idx = 256 + i\n",
    "            # replace all occurrences of pair in ids with idx\n",
    "            ids = [merge(chunk_ids, pair, idx) for chunk_ids in ids]\n",
    "            # save the merge\n",
    "            merges[pair] = idx\n",
    "            vocab[idx] = vocab[pair[0]] + vocab[pair[1]]\n",
    "            # prints\n",
    "            if verbose:\n",
    "                print(f\"merge {i+1}/{num_merges}: {pair} -> {idx} ({vocab[idx]}) had {stats[pair]} occurrences\")\n",
    "\n",
    "        # save class variables\n",
    "        self.merges = merges # used in encode()\n",
    "        self.vocab = vocab   # used in decode()\n",
    "        self.train_ids_count = len(ids)\n",
    "\n",
    "    def register_special_tokens(self, special_tokens):\n",
    "        # special_tokens is a dictionary of str -> int\n",
    "        # example: {\"<|endoftext|>\": 100257}\n",
    "        self.special_tokens = special_tokens\n",
    "        self.inverse_special_tokens = {v: k for k, v in special_tokens.items()}\n",
    "\n",
    "    def decode(self, ids):\n",
    "        # given ids (list of integers), return Python string\n",
    "        part_bytes = []\n",
    "        for idx in ids:\n",
    "            if idx in self.vocab:\n",
    "                part_bytes.append(self.vocab[idx])\n",
    "            elif idx in self.inverse_special_tokens:\n",
    "                part_bytes.append(self.inverse_special_tokens[idx].encode(\"utf-8\"))\n",
    "            else:\n",
    "                raise ValueError(f\"invalid token id: {idx}\")\n",
    "        text_bytes = b\"\".join(part_bytes)\n",
    "        text = text_bytes.decode(\"utf-8\", errors=\"replace\")\n",
    "        return text\n",
    "\n",
    "    def _encode_chunk(self, text_bytes):\n",
    "        # return the token ids\n",
    "        # let's begin. first, convert all bytes to integers in range 0..255\n",
    "        ids = list(text_bytes)\n",
    "        while len(ids) >= 2:\n",
    "            # find the pair with the lowest merge index\n",
    "            stats = get_stats(ids)\n",
    "            pair = min(stats, key=lambda p: self.merges.get(p, float(\"inf\")))\n",
    "            # subtle: if there are no more merges available, the key will\n",
    "            # result in an inf for every single pair, and the min will be\n",
    "            # just the first pair in the list, arbitrarily\n",
    "            # we can detect this terminating case by a membership check\n",
    "            if pair not in self.merges:\n",
    "                break # nothing else can be merged anymore\n",
    "            # otherwise let's merge the best pair (lowest merge index)\n",
    "            idx = self.merges[pair]\n",
    "            ids = merge(ids, pair, idx)\n",
    "        return ids\n",
    "\n",
    "    def encode_ordinary(self, text):\n",
    "        \"\"\"Encoding that ignores any special tokens.\"\"\"\n",
    "        # split text into chunks of text by categories defined in regex pattern\n",
    "        text_chunks = re.findall(self.compiled_pattern, text)\n",
    "        # all chunks of text are encoded separately, then results are joined\n",
    "        ids = []\n",
    "        for chunk in text_chunks:\n",
    "            chunk_bytes = chunk.encode(\"utf-8\") # raw bytes\n",
    "            chunk_ids = self._encode_chunk(chunk_bytes)\n",
    "            ids.extend(chunk_ids)\n",
    "        return ids\n",
    "\n",
    "    def encode(self, text, allowed_special=\"none_raise\"):\n",
    "        \"\"\"\n",
    "        Unlike encode_ordinary, this function handles special tokens.\n",
    "        allowed_special: can be \"all\"|\"none\"|\"none_raise\" or a custom set of special tokens\n",
    "        if none_raise, then an error is raised if any special token is encountered in text\n",
    "        this is the default tiktoken behavior right now as well\n",
    "        any other behavior is either annoying, or a major footgun\n",
    "        \"\"\"\n",
    "        # decode the user desire w.r.t. handling of special tokens\n",
    "        special = None\n",
    "        if allowed_special == \"all\":\n",
    "            special = self.special_tokens\n",
    "        elif allowed_special == \"none\":\n",
    "            special = {}\n",
    "        elif allowed_special == \"none_raise\":\n",
    "            special = {}\n",
    "            assert all(token not in text for token in self.special_tokens)\n",
    "        elif isinstance(allowed_special, set):\n",
    "            special = {k: v for k, v in self.special_tokens.items() if k in allowed_special}\n",
    "        else:\n",
    "            raise ValueError(f\"allowed_special={allowed_special} not understood\")\n",
    "        if not special:\n",
    "            # shortcut: if no special tokens, just use the ordinary encoding\n",
    "            return self.encode_ordinary(text)\n",
    "        # otherwise, we have to be careful with potential special tokens in text\n",
    "        # we handle special tokens by splitting the text\n",
    "        # based on the occurrence of any exact match with any of the special tokens\n",
    "        # we can use re.split for this. note that surrounding the pattern with ()\n",
    "        # makes it into a capturing group, so the special tokens will be included\n",
    "        special_pattern = \"(\" + \"|\".join(re.escape(k) for k in special) + \")\"\n",
    "        special_chunks = re.split(special_pattern, text)\n",
    "        # now all the special characters are separated from the rest of the text\n",
    "        # all chunks of text are encoded separately, then results are joined\n",
    "        ids = []\n",
    "        for part in special_chunks:\n",
    "            if part in special:\n",
    "                # this is a special token, encode it separately as a special case\n",
    "                ids.append(special[part])\n",
    "            else:\n",
    "                # this is an ordinary sequence, encode it normally\n",
    "                ids.extend(self.encode_ordinary(part))\n",
    "        return ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text length: 260799, Total token length: 661827, First 20 tokens: [224, 164, 173, 224, 164, 190, 224, 164, 176, 224, 164, 164, 32, 224, 164, 149, 224, 165, 135, 32]...\n",
      "merge 1/50000: (224, 164) -> 256 (b'\\xe0\\xa4') had 155208 occurrences\n",
      "merge 2/50000: (224, 165) -> 257 (b'\\xe0\\xa5') had 44662 occurrences\n",
      "merge 3/50000: (32, 256) -> 258 (b' \\xe0\\xa4') had 39129 occurrences\n",
      "merge 4/50000: (256, 190) -> 259 (b'\\xe0\\xa4\\xbe') had 17822 occurrences\n",
      "merge 5/50000: (257, 141) -> 260 (b'\\xe0\\xa5\\x8d') had 12464 occurrences\n",
      "merge 6/50000: (260, 256) -> 261 (b'\\xe0\\xa5\\x8d\\xe0\\xa4') had 12237 occurrences\n",
      "merge 7/50000: (259, 256) -> 262 (b'\\xe0\\xa4\\xbe\\xe0\\xa4') had 11659 occurrences\n",
      "merge 8/50000: (257, 135) -> 263 (b'\\xe0\\xa5\\x87') had 11223 occurrences\n",
      "merge 9/50000: (256, 191) -> 264 (b'\\xe0\\xa4\\xbf') had 8463 occurrences\n",
      "merge 10/50000: (258, 149) -> 265 (b' \\xe0\\xa4\\x95') had 7614 occurrences\n",
      "merge 11/50000: (264, 256) -> 266 (b'\\xe0\\xa4\\xbf\\xe0\\xa4') had 7286 occurrences\n",
      "merge 12/50000: (256, 130) -> 267 (b'\\xe0\\xa4\\x82') had 6763 occurrences\n",
      "merge 13/50000: (257, 128) -> 268 (b'\\xe0\\xa5\\x80') had 6660 occurrences\n",
      "merge 14/50000: (256, 176) -> 269 (b'\\xe0\\xa4\\xb0') had 6262 occurrences\n",
      "merge 15/50000: (257, 139) -> 270 (b'\\xe0\\xa5\\x8b') had 4808 occurrences\n",
      "merge 16/50000: (256, 164) -> 271 (b'\\xe0\\xa4\\xa4') had 4306 occurrences\n",
      "merge 17/50000: (256, 168) -> 272 (b'\\xe0\\xa4\\xa8') had 4262 occurrences\n",
      "merge 18/50000: (256, 149) -> 273 (b'\\xe0\\xa4\\x95') had 4011 occurrences\n",
      "merge 19/50000: (258, 184) -> 274 (b' \\xe0\\xa4\\xb8') had 3761 occurrences\n",
      "merge 20/50000: (256, 185) -> 275 (b'\\xe0\\xa4\\xb9') had 3544 occurrences\n",
      "merge 21/50000: (256, 184) -> 276 (b'\\xe0\\xa4\\xb8') had 3011 occurrences\n",
      "merge 22/50000: (258, 174) -> 277 (b' \\xe0\\xa4\\xae') had 2959 occurrences\n",
      "merge 23/50000: (261, 176) -> 278 (b'\\xe0\\xa5\\x8d\\xe0\\xa4\\xb0') had 2814 occurrences\n",
      "merge 24/50000: (258, 170) -> 279 (b' \\xe0\\xa4\\xaa') had 2776 occurrences\n",
      "merge 25/50000: (257, 136) -> 280 (b'\\xe0\\xa5\\x88') had 2656 occurrences\n",
      "merge 26/50000: (257, 129) -> 281 (b'\\xe0\\xa5\\x81') had 2491 occurrences\n",
      "merge 27/50000: (256, 174) -> 282 (b'\\xe0\\xa4\\xae') had 2396 occurrences\n",
      "merge 28/50000: (256, 178) -> 283 (b'\\xe0\\xa4\\xb2') had 2367 occurrences\n",
      "merge 29/50000: (263, 267) -> 284 (b'\\xe0\\xa5\\x87\\xe0\\xa4\\x82') had 2277 occurrences\n",
      "merge 30/50000: (262, 176) -> 285 (b'\\xe0\\xa4\\xbe\\xe0\\xa4\\xb0') had 2204 occurrences\n",
      "merge 31/50000: (256, 166) -> 286 (b'\\xe0\\xa4\\xa6') had 2182 occurrences\n",
      "merge 32/50000: (265, 263) -> 287 (b' \\xe0\\xa4\\x95\\xe0\\xa5\\x87') had 2181 occurrences\n",
      "merge 33/50000: (261, 175) -> 288 (b'\\xe0\\xa5\\x8d\\xe0\\xa4\\xaf') had 1969 occurrences\n",
      "merge 34/50000: (256, 170) -> 289 (b'\\xe0\\xa4\\xaa') had 1854 occurrences\n",
      "merge 35/50000: (258, 185) -> 290 (b' \\xe0\\xa4\\xb9') had 1854 occurrences\n",
      "merge 36/50000: (258, 181) -> 291 (b' \\xe0\\xa4\\xb5') had 1714 occurrences\n",
      "merge 37/50000: (256, 151) -> 292 (b'\\xe0\\xa4\\x97') had 1639 occurrences\n",
      "merge 38/50000: (270, 267) -> 293 (b'\\xe0\\xa5\\x8b\\xe0\\xa4\\x82') had 1636 occurrences\n",
      "merge 39/50000: (277, 284) -> 294 (b' \\xe0\\xa4\\xae\\xe0\\xa5\\x87\\xe0\\xa4\\x82') had 1616 occurrences\n",
      "merge 40/50000: (256, 175) -> 295 (b'\\xe0\\xa4\\xaf') had 1562 occurrences\n",
      "merge 41/50000: (256, 181) -> 296 (b'\\xe0\\xa4\\xb5') had 1548 occurrences\n",
      "merge 42/50000: (258, 156) -> 297 (b' \\xe0\\xa4\\x9c') had 1542 occurrences\n",
      "merge 43/50000: (258, 168) -> 298 (b' \\xe0\\xa4\\xa8') had 1511 occurrences\n",
      "merge 44/50000: (275, 280) -> 299 (b'\\xe0\\xa4\\xb9\\xe0\\xa5\\x88') had 1439 occurrences\n",
      "merge 45/50000: (262, 168) -> 300 (b'\\xe0\\xa4\\xbe\\xe0\\xa4\\xa8') had 1416 occurrences\n",
      "merge 46/50000: (258, 133) -> 301 (b' \\xe0\\xa4\\x85') had 1395 occurrences\n",
      "merge 47/50000: (258, 172) -> 302 (b' \\xe0\\xa4\\xac') had 1372 occurrences\n",
      "merge 48/50000: (257, 130) -> 303 (b'\\xe0\\xa5\\x82') had 1343 occurrences\n",
      "merge 49/50000: (257, 164) -> 304 (b'\\xe0\\xa5\\xa4') had 1331 occurrences\n",
      "merge 50/50000: (261, 164) -> 305 (b'\\xe0\\xa5\\x8d\\xe0\\xa4\\xa4') had 1257 occurrences\n",
      "merge 51/50000: (266, 149) -> 306 (b'\\xe0\\xa4\\xbf\\xe0\\xa4\\x95') had 1250 occurrences\n",
      "merge 52/50000: (266, 175) -> 307 (b'\\xe0\\xa4\\xbf\\xe0\\xa4\\xaf') had 1150 occurrences\n",
      "merge 53/50000: (256, 165) -> 308 (b'\\xe0\\xa4\\xa5') had 1123 occurrences\n",
      "merge 54/50000: (265, 268) -> 309 (b' \\xe0\\xa4\\x95\\xe0\\xa5\\x80') had 1123 occurrences\n",
      "merge 55/50000: (263, 256) -> 310 (b'\\xe0\\xa5\\x87\\xe0\\xa4') had 1108 occurrences\n",
      "merge 56/50000: (258, 166) -> 311 (b' \\xe0\\xa4\\xa6') had 1072 occurrences\n",
      "merge 57/50000: (269, 261) -> 312 (b'\\xe0\\xa4\\xb0\\xe0\\xa5\\x8d\\xe0\\xa4') had 1049 occurrences\n",
      "merge 58/50000: (279, 278) -> 313 (b' \\xe0\\xa4\\xaa\\xe0\\xa5\\x8d\\xe0\\xa4\\xb0') had 1010 occurrences\n",
      "merge 59/50000: (258, 176) -> 314 (b' \\xe0\\xa4\\xb0') had 992 occurrences\n",
      "merge 60/50000: (258, 173) -> 315 (b' \\xe0\\xa4\\xad') had 988 occurrences\n",
      "merge 61/50000: (258, 137) -> 316 (b' \\xe0\\xa4\\x89') had 946 occurrences\n",
      "merge 62/50000: (262, 178) -> 317 (b'\\xe0\\xa4\\xbe\\xe0\\xa4\\xb2') had 936 occurrences\n",
      "merge 63/50000: (258, 164) -> 318 (b' \\xe0\\xa4\\xa4') had 927 occurrences\n",
      "merge 64/50000: (256, 172) -> 319 (b'\\xe0\\xa4\\xac') had 912 occurrences\n",
      "merge 65/50000: (265, 259) -> 320 (b' \\xe0\\xa4\\x95\\xe0\\xa4\\xbe') had 908 occurrences\n",
      "merge 66/50000: (256, 154) -> 321 (b'\\xe0\\xa4\\x9a') had 892 occurrences\n",
      "merge 67/50000: (258, 148) -> 322 (b' \\xe0\\xa4\\x94') had 888 occurrences\n",
      "merge 68/50000: (322, 269) -> 323 (b' \\xe0\\xa4\\x94\\xe0\\xa4\\xb0') had 867 occurrences\n",
      "merge 69/50000: (274, 263) -> 324 (b' \\xe0\\xa4\\xb8\\xe0\\xa5\\x87') had 862 occurrences\n",
      "merge 70/50000: (256, 167) -> 325 (b'\\xe0\\xa4\\xa7') had 844 occurrences\n",
      "merge 71/50000: (266, 164) -> 326 (b'\\xe0\\xa4\\xbf\\xe0\\xa4\\xa4') had 829 occurrences\n",
      "merge 72/50000: (258, 178) -> 327 (b' \\xe0\\xa4\\xb2') had 825 occurrences\n",
      "merge 73/50000: (258, 151) -> 328 (b' \\xe0\\xa4\\x97') had 815 occurrences\n",
      "merge 74/50000: (258, 134) -> 329 (b' \\xe0\\xa4\\x86') had 790 occurrences\n",
      "merge 75/50000: (281, 256) -> 330 (b'\\xe0\\xa5\\x81\\xe0\\xa4') had 781 occurrences\n",
      "merge 76/50000: (262, 164) -> 331 (b'\\xe0\\xa4\\xbe\\xe0\\xa4\\xa4') had 777 occurrences\n",
      "merge 77/50000: (265, 270) -> 332 (b' \\xe0\\xa4\\x95\\xe0\\xa5\\x8b') had 769 occurrences\n",
      "merge 78/50000: (271, 259) -> 333 (b'\\xe0\\xa4\\xa4\\xe0\\xa4\\xbe') had 750 occurrences\n",
      "merge 79/50000: (256, 188) -> 334 (b'\\xe0\\xa4\\xbc') had 733 occurrences\n",
      "merge 80/50000: (256, 161) -> 335 (b'\\xe0\\xa4\\xa1') had 717 occurrences\n",
      "merge 81/50000: (258, 175) -> 336 (b' \\xe0\\xa4\\xaf') had 696 occurrences\n",
      "merge 82/50000: (256, 156) -> 337 (b'\\xe0\\xa4\\x9c') had 691 occurrences\n",
      "merge 83/50000: (261, 181) -> 338 (b'\\xe0\\xa5\\x8d\\xe0\\xa4\\xb5') had 685 occurrences\n",
      "merge 84/50000: (262, 175) -> 339 (b'\\xe0\\xa4\\xbe\\xe0\\xa4\\xaf') had 683 occurrences\n",
      "merge 85/50000: (256, 173) -> 340 (b'\\xe0\\xa4\\xad') had 680 occurrences\n",
      "merge 86/50000: (262, 156) -> 341 (b'\\xe0\\xa4\\xbe\\xe0\\xa4\\x9c') had 670 occurrences\n",
      "merge 87/50000: (272, 263) -> 342 (b'\\xe0\\xa4\\xa8\\xe0\\xa5\\x87') had 655 occurrences\n",
      "merge 88/50000: (258, 135) -> 343 (b' \\xe0\\xa4\\x87') had 653 occurrences\n",
      "merge 89/50000: (32, 10) -> 344 (b' \\n') had 641 occurrences\n",
      "merge 90/50000: (226, 128) -> 345 (b'\\xe2\\x80') had 624 occurrences\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[60], line 16\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# construct the Tokenizer object and kick off verbose training\u001b[39;00m\n\u001b[0;32m     15\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m RegexTokenizer(compiled_pattern_5)\n\u001b[1;32m---> 16\u001b[0m \u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mVOCAB_SIZE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# writes two files in the models directory: name.model, and name.vocab\u001b[39;00m\n\u001b[0;32m     18\u001b[0m tokenizer\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mregex-hindi\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[59], line 47\u001b[0m, in \u001b[0;36mRegexTokenizer.train\u001b[1;34m(self, text, vocab_size, verbose)\u001b[0m\n\u001b[0;32m     45\u001b[0m idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m256\u001b[39m \u001b[38;5;241m+\u001b[39m i\n\u001b[0;32m     46\u001b[0m \u001b[38;5;66;03m# replace all occurrences of pair in ids with idx\u001b[39;00m\n\u001b[1;32m---> 47\u001b[0m ids \u001b[38;5;241m=\u001b[39m [merge(chunk_ids, pair, idx) \u001b[38;5;28;01mfor\u001b[39;00m chunk_ids \u001b[38;5;129;01min\u001b[39;00m ids]\n\u001b[0;32m     48\u001b[0m \u001b[38;5;66;03m# save the merge\u001b[39;00m\n\u001b[0;32m     49\u001b[0m merges[pair] \u001b[38;5;241m=\u001b[39m idx\n",
      "Cell \u001b[1;32mIn[59], line 47\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     45\u001b[0m idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m256\u001b[39m \u001b[38;5;241m+\u001b[39m i\n\u001b[0;32m     46\u001b[0m \u001b[38;5;66;03m# replace all occurrences of pair in ids with idx\u001b[39;00m\n\u001b[1;32m---> 47\u001b[0m ids \u001b[38;5;241m=\u001b[39m [\u001b[43mmerge\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunk_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpair\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m chunk_ids \u001b[38;5;129;01min\u001b[39;00m ids]\n\u001b[0;32m     48\u001b[0m \u001b[38;5;66;03m# save the merge\u001b[39;00m\n\u001b[0;32m     49\u001b[0m merges[pair] \u001b[38;5;241m=\u001b[39m idx\n",
      "Cell \u001b[1;32mIn[57], line 26\u001b[0m, in \u001b[0;36mmerge\u001b[1;34m(ids, pair, idx)\u001b[0m\n\u001b[0;32m     24\u001b[0m newids \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     25\u001b[0m i \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m---> 26\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m i \u001b[38;5;241m<\u001b[39m \u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mids\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m     27\u001b[0m     \u001b[38;5;66;03m# if not at the very last position AND the pair matches, replace it\u001b[39;00m\n\u001b[0;32m     28\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ids[i] \u001b[38;5;241m==\u001b[39m pair[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;129;01mand\u001b[39;00m i \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mlen\u001b[39m(ids) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m ids[i\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m pair[\u001b[38;5;241m1\u001b[39m]:\n\u001b[0;32m     29\u001b[0m         newids\u001b[38;5;241m.\u001b[39mappend(idx)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "VOCAB_SIZE = 5256\n",
    "\n",
    "# open some text and train a vocab of 512 tokens\n",
    "text = get_file_input()\n",
    "\n",
    "initial_tokens = file_content.encode(\"utf-8\")\n",
    "initial_tokens = list(map(int, tokens))\n",
    "print(f\"Text length: {len(file_content)}, Total token length: {len(initial_tokens)}, First 20 tokens: {initial_tokens[:20]}...\")\n",
    "\n",
    "t0 = time.time()\n",
    "# construct the Tokenizer object and kick off verbose training\n",
    "tokenizer = RegexTokenizer(compiled_pattern_5)\n",
    "tokenizer.train(text, VOCAB_SIZE, verbose=True)\n",
    "# writes two files in the models directory: name.model, and name.vocab\n",
    "tokenizer.save(\"regex-hindi\")\n",
    "t1 = time.time()\n",
    "print(f\"Training took {t1 - t0:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"tokens length:\", len(initial_tokens))\n",
    "print(\"ids length:\", tokenizer.train_ids_count)\n",
    "print(f\"compression ratio: {len(tokens) / tokenizer.train_ids_count:.2f}X\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_text = \"राष्ट्र गान के बारे में समय समय पर अनुदेश जारी किए गए हैं । पुराने विचारों के हिन्दू ‘रामायण’ को “त्रेता” युग की तथा महाभारत को “द्वापर” युग की कृति मानते हैं ।\"\n",
    "ids = tokenizer.encode(text)\n",
    "print(ids)\n",
    "decoded = tokenizer.decode(ids)\n",
    "print(decoded)\n",
    "assert text == decoded"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cuda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
